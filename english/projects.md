---
layout: page
---

<h3>Application of Tele-Robotic Technology for Auxiliary Diagnosis and Treatment of COVID−19 in Isolation Ward</h3>
<p>
This project conducts the teleoperation of the dual-arm robot YuMi base the multi-node inertial sensing motion capture technology, which can mapping the diagnosis and treatment operation behavior of the doctors and caregivers. This project aims to minimize the need for medical staff into the isolation ward, and reducing the cross-infection between the COVID-19 patients and the doctors or the caregivers. At present, the hardware construction and hardware debugging of the assembly platform have been completed, and the preliminary clinical verification has been carried out. In view of the related reasons, the video, pictures and related codes will be released at the right time.
</p>

<h3>AHuman-robot Collaborative Assembly Based on Target Recognition Using Kinect</h3>
<p>
In this project, I use the Depth camera kinectV2 to obtain the 2D position coordinate information and rotation information of the target object in the assembly workspace (at present, the rotation information of the object in 2D plane has only one variable - Euler angle around Z axis). Then the dual-arm collaborative robot YuMi grab the object using the position and orientation data after coordinate transformation. After grabbing, YuMi place the object to the appropriate position pre-setted for assembly process,  while the human in the same workspace also do other details work such as assembly screws at the same time. The project is aim to improve the working efficiency and give full play to the functional advantages of human and robot. At present, the hardware of the assembly platform has been built and debugged. The following is the video demonstration of this design work. 
</p>
<table>
<tr>
<td> <iframe text-align="center" src="//player.bilibili.com/player.html?aid=53345688&cid=93330430&page=1" scrolling="yes" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="100%"> </iframe> </td>
</tr>
</table>

<h3>Design and Security Interaction of the Collaborative Mobile Platform Based on YuMi Robot</h3>
<p>
This project is the graduation design for my bachelor degree. Based on ABB's dual arm cooperative robot--YuMi, this paper designs a cooperative mobile platform for helping the elderly and disabled people and medical care. This design includes the mechanical structure design, electrical system design and the programming of the software program. The 3D modeling and physical construction building have been completed, and the simulation and debugging of the software program have been carried out. This design is safe, efficient, convenient and practical. Below is the video demo of this project.
</p>
<table>
<tr>
<td> <iframe text-align="center" src="//player.bilibili.com/player.html?aid=53345688&cid=93330430&page=1" scrolling="yes" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="100%"> </iframe> </td>
</tr>
</table>

<h3>A Novel Gesture Recognition System for Intelligent Interaction with a Nursing-Care Assistant Robot</h3>
<p>
This project proposed an innovative hand motion trajectory (HMT) gesture recognition system based on background velocity features. Here, a new wearable wrist-worn camera prototype for gesture’s video collection was designed, and a new method for the segmentation of continuous gestures was shown. Meanwhile, a nursing-care assistant robot prototype was designed for assisting the elderly, which is capable of carrying the elderly with omnidirectional motion and grabbing the specified object at home. In order to evaluate the performance of the gesture recognition system, 10 special gestures were defined as the move commands for interaction with the robot. The hardware part of wearable wrist camera is completed independently by Chen Feiyu, and the software program is modified based on Chen Feiyu's code foundation, and then connects with the interface of nursing-care robot in my graduation design project to realize gesture control. In this project, ten kinds of gesture commands are defined to control the movement of the robot. Below is the video demo of this project. 
</p>
<table>
<tr>
<td> <iframe text-align="center" src="//player.bilibili.com/player.html?aid=81541015&cid=139654689&page=1" scrolling="yes" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="100%"> </iframe> </td>
</tr>
</table>